import string
import json
import os
import re
import nltk

# Set max number of tokens allowed in a sentence.
# Sentences above this limit are completely excluded
# from the training data.
MAX_NUM_TOKENS = 50

# Set the maximum number of utterances to load.
MAX_NUM_UTTERANCES = 5000

# If specifified, only tweets from this user name will be used as replies.
TARGET_USER = None


def clean_content(content):
    ''' Cleans the text belonging to a content in the Facebook data. '''
    # Facebook encodes the data in their exports incorrectly. We can work around
    # this error by encoding the data as Latin-1 and decoding again as UTF-8.
    content = content.encode('latin1').decode('utf8')
    # Convert all text to lowercase.
    content = content.lower()
    # Remove all punctuation from text.
    content = re.sub('[{}]'.format(re.escape(string.punctuation)), '', content)
    # Replace newlines with spaces.
    content = re.sub('\n', ' ', content)
    # Return the cleaned content.
    return content


def load_utterances():
    ''' Load a list of utterances from Facebook data. '''
    utterances = []

    # Recursively traverse all directories in the corpus folder.
    for root, subdirs, files in os.walk('corpus'):
        # Traverse all files found in this subdirectory.
        for filename in files:
            # Check if we found a JSON file.
            if filename.endswith('json'):
                # Find the complete file path.
                file_path = os.path.join(root, filename)
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    # Load the data file.
                    data = json.load(f)

                    for message in data.get('messages', []):
                        if 'content' in message:
                            utterances.append({
                                'sender_name': clean_content(message['sender_name']),
                                'content': clean_content(message['content']),
                            })

    return utterances


def wrap_utterance(utterance):
    ''' Wrap an utterance in start and end tags, to detect when
    an entire utterance has been generated by the chatbot. '''
    return ['<u>'] + utterance + ['</u>']


def tokenize(utterance):
    ''' Tokenize and clean an utterance. '''
    # Tokenize the utterance using NLTK.
    tokens = [
        token for sentence in nltk.sent_tokenize(utterance, language='norwegian')
        for token in nltk.word_tokenize(sentence, language='norwegian')[:MAX_NUM_TOKENS]
    ]

    # Return tokenized utterance.
    return tokens


def verify_utterance(tokens):
    ''' Verify the quality of an utterance before including it in the dataset. '''
    return len(tokens) <= MAX_NUM_TOKENS


def get_utterance_pairs():
    ''' Load utterances and split them into questions and answers. '''
    # Load utterances from file.
    utterances = load_utterances()[:MAX_NUM_UTTERANCES]

    # Lists for input utterances with corresponding output utterances.
    input_utterances, target_utterances = [], []

    # Loop through all utterances, starting at the second line.
    for i, utterance in enumerate(utterances[1:], 1):
        # Tokenize input and target utterances.
        input_tokens, target_tokens = map(tokenize, (utterances[i-1]['content'], utterance['content']))

        # Check if both the input and the target utterances are good enough to use.
        # Also check that the user of the target message is the target user, if set.
        if not (verify_utterance(input_tokens) and verify_utterance(target_tokens)) or \
                (TARGET_USER != None and utterance['sender_name'] != TARGET_USER.lower()):
            continue

        # Add input utterance to list.
        input_utterances.append(input_tokens)

        # Add corresponding output utterance.
        target_utterances.append(wrap_utterance(target_tokens))

    return input_utterances, target_utterances


def get_word_map(corpus):
    ''' Create mapping between tokens and an unique number for each
    token, and vice versa. '''
    # Find tokens from all utterances in the corpus.
    tokens = set(token for utterance in corpus for token in utterance)
    # Map tokens to an unique number. Start at 1 to keep 0 as padding
    token_to_num = { token: i for i, token in enumerate(tokens, start=1) }
    # Inverse mapping which takes numbers back to tokens.
    num_to_token = { i: token for token, i in token_to_num.items() }
    # Return both mappings.
    return token_to_num, num_to_token
