import string
import json
import os
import re
import nltk

# Set max number of tokens in a sentence.
MAX_NUM_TOKENS = 200

# Set the maximum number of utterances to load.
MAX_NUM_UTTERANCES = 5000

def clean_content(content):
    ''' Cleans the text belonging to a content in the Facebook data. '''
    # Facebook encodes the data in their exports incorrectly. We can work around
    # this error by encoding the data as Latin-1 and decoding again as UTF-8.
    content = content.encode('latin1').decode('utf8')
    # Convert all text to lowercase.
    content = content.lower()
    # Remove all punctuation from text.
    content = re.sub('[{}]'.format(re.escape(string.punctuation)), '', content)
    # Replace newlines with spaces.
    content = re.sub('\n', ' ', content)
    # Return the cleaned content.
    return content


def load_utterances():
    ''' Load a list of utterances from Facebook data. '''
    utterances = []

    # Recursively traverse all directories in the corpus folder.
    for root, subdirs, files in os.walk('corpus'):
        # Traverse all files found in this subdirectory.
        for filename in files:
            # Check if we found a JSON file.
            if filename.endswith('json'):
                # Find the complete file path.
                file_path = os.path.join(root, filename)
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    # Load the data file.
                    data = json.load(f)

                    if 'messages' in data:
                        # Extend the list of utterances with the new data.
                        utterances.extend(
                            clean_content(message['content']) 
                            for message in data['messages'] 
                            if 'content' in message
                        )

    return utterances


def wrap_utterance(utterance):
    ''' Wrap an utterance in start and end tags, to detect when
    an entire utterance has been generated by the chatbot. '''
    return ['<u>'] + utterance + ['</u>']


def tokenize(utterance):
    ''' Tokenize and clean an utterance. '''
    # Tokenize the utterance using NLTK.
    tokens = [
        token for sentence in nltk.sent_tokenize(utterance, language='norwegian')
        for token in nltk.word_tokenize(sentence, language='norwegian')[:MAX_NUM_TOKENS]
    ]

    # Return tokenized utterance.
    return tokens


def get_utterance_pairs():
  ''' Load utterances and split them into questions and answers. '''
  # Load utterances from file.
  lines = load_utterances()[:MAX_NUM_UTTERANCES]
 
  # Lists for input utterances with corresponding output utterances.
  input_utterances, target_utterances = [], []

  # Loop through all lines, starting at the second line.
  for i, line in enumerate(lines[1:], 1):
      # Add input utterance to list.
      input_utterances.append(tokenize(lines[i-1]))

      # Add corresponding output utterance.
      target_utterances.append(wrap_utterance(tokenize(line)))

  return input_utterances, target_utterances


def get_word_map(corpus):
    ''' Create mapping between tokens and an unique number for each
    token, and vice versa. '''
    # Find tokens from all utterances in the corpus.
    tokens = set(token for utterance in corpus for token in utterance)
    # Map tokens to an unique number.
    token_to_num = { token: i for i, token in enumerate(tokens) }
    # Inverse mapping which takes numbers back to tokens.
    num_to_token = { i: token for token, i in token_to_num.items() }
    # Return both mappings.
    return token_to_num, num_to_token
